# -*- coding: utf-8 -*-
"""Landmark_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j-0ivD2KmFEx8-1MXytNN4emsVgVTweY
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Mount Drive and set correct paths**"""

from google.colab import drive
import tarfile
import os

# Mount Google Drive (for the .tar file)
drive.mount('/content/drive')

# Path to .tar file in your Drive
tar_path = "/content/drive/MyDrive/Landmark_data/images_000.tar"

# Path to extract images
base_path = "/content/images/"  # This matches your original code
if not os.path.exists(base_path):
    os.makedirs(base_path)

# Extract only if folder is empty
if not os.listdir(base_path):
    with tarfile.open(tar_path) as tar:
        tar.extractall(path=base_path)
        print("Tar file extracted successfully.")
else:
    print("Images already extracted.")

import pandas as pd

df = pd.read_csv("train.csv")
print(df.head())

"""** IMport Libraries**"""

# ================================
# Step 1 – Import Libraries
# ================================
import numpy as np
import pandas as pd
import cv2
import os
import random
from PIL import Image

import keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

"""**dataset**"""

# ================================
# Step 2 – Load Dataset
# ================================
df = pd.read_csv("train.csv")   # Make sure train.csv is uploaded in Colab

"""**Sample Dataset Info**"""

num_classes = len(df["landmark_id"].unique())
num_data = len(df)
print("Number of classes:", num_classes)
print("Number of data points:", num_data)

data=pd.DataFrame(df["landmark_id"].value_counts())
data.reset_index(inplace=True)
data.head()

"""**Preprocessing Imagest**"""

def load_images(ids, labels, base_path, img_size=(64,64)):
    images = []
    y = []
    for i, img_id in enumerate(ids):
        folder = img_id[0:3]   # first 3 chars for folder
        path = os.path.join(base_path, folder, img_id + ".jpg")
        if os.path.exists(path):
            try:
                img = Image.open(path).convert("RGB")
                img = img.resize(img_size)
                img = np.array(img) / 255.0
                images.append(img)
                y.append(labels[i])
            except:
                pass
    return np.array(images), np.array(y)

# Take a smaller sample for faster training
sample_df = df.sample(20000, random_state=42)
X, y = load_images(sample_df["id"].values, sample_df["landmark_id"].values, base_path)

print("Image data shape:", X.shape)
print("Labels shape:", y.shape)

"""**Encode Labels**"""

encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

print("Training data:", X_train.shape, y_train.shape)
print("Validation data:", X_val.shape, y_val.shape)

"""**Build CNN Model**"""

model = Sequential([
    Conv2D(32, (3,3), activation="relu", input_shape=(64,64,3)),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(64, (3,3), activation="relu"),
    MaxPooling2D(pool_size=(2,2)),

    Flatten(),
    Dense(128, activation="relu"),
    Dropout(0.5),
    Dense(num_classes, activation="softmax")
])

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

"""**Train Model**"""

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10, batch_size=64
)

"""**Training Graphs**"""

plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.legend()
plt.title("Training vs Validation Loss")
plt.show()

plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.legend()
plt.title("Training vs Validation Accuracy")
plt.show()

"""**Predictions on Sample Images**"""

indices = np.random.choice(len(X_val), 5)
sample_images = X_val[indices]
sample_labels = y_val[indices]

predictions = model.predict(sample_images)
pred_classes = np.argmax(predictions, axis=1)

for i in range(len(sample_images)):
    plt.imshow(sample_images[i])
    plt.title(f"Predicted: {pred_classes[i]}, Actual: {sample_labels[i]}")
    plt.axis("off")
    plt.show()

"""**Model Evaluatio**"""

y_pred = model.predict(X_val)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = y_val

print("Classification Report:")
print(classification_report(y_true, y_pred_classes))

cm = confusion_matrix(y_true, y_pred_classes)

plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=False, cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **Save Model**"""

model.save("landmark_detection_model.h5")
print("Model saved successfully as landmark_detection_model.h5")

"""# WHat it not COVER
**What it does NOT cover

Handling full dataset with millions of images (we are using a small sample of 20,000 for speed).

Advanced data augmentation (flips, rotations) — helps if accuracy is low.

Using GPU efficiently for huge datasets (Colab free GPU may run out of memory for full dataset).

Test set predictions (only train-validation split is used here).**
"""

